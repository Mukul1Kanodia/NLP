# -*- coding: utf-8 -*-
"""TextBlob.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YEFxgV66rIOSjaE7tgeKajYgGoJ5vTSV
"""

!pip install nltk
!pip install textblob

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('popular')

from textblob import TextBlob
blob=TextBlob("Hey John, How are you!")
print("Detected language is:",blob.detect_language())
print("Text in spanish:",blob.translate(to='es'))

!python -m textblob.download_corpora

from textblob import TextBlob
text=""" ABCD Corp alays values ttheir employees!!!"""

blob=TextBlob(text)

blob

blob.correct()

TextBlob('hasss').correct()

TextBlob('myyyy').correct()

"""Word **Count**"""

text="Sentiment Analysis is a process by which we can find the sentiment of a text. Sentiment can be Positive, Negative or Neutral"
blob=TextBlob(text)
blob.word_counts["analysis"]

blob.word_counts["Sentiment"]

blob.word_counts["sentiment"]

blob.word_counts["Analysis"]

"""# POS Tagging
With the help of tags function of textblob, we can get tag each words of a sentence with a tag that can be either noun, pronoun, verb, adverb, adjective and more.
"""

from textblob import TextBlob

text = TextBlob("My name is Adam. I like to read about NLP. I work at ABCD Corp.")
print(text.tags)

new_tuple=[]
for i in text.tags:
    print(i)
    if 'VBP' not in i[1]:
        new_tuple.append(i)

new_tuple

value=''
for i in new_tuple:
  value=value+" "+''.join(i[0])

print(value)

"""**Tokenization**


Corpus (or corpora in plural) - Corpus is nothing but a collection of text data. The text maybe in one language or maybe a combination of two or more.

Token - The term "Token" is nothing but the total number of words in a text, corpus etc, regardless of their freuqncy of occurrence in the text. Tokens are nothing but a string of contiguous characters which either lies between the two spaces or it lies between a space and punctuation. For Example: Suppose you have the following string : "abc_123_defg", if you split it on basis of underscores "_" you obtained three tokens : "abc", "123" and "defg".

What is tokenization?

Tokenization is a process of splitting the sentence or corpus into its smalles unit i.e. "Tokens"
"""

text="""
R is a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts. It is free and runs on a variety of platforms, including Windows, Unix, and macOS. It provides an unparalleled platform for programming new statistical methods in an easy and straightforward manner.
"""
blob_object = TextBlob(text)
# Word tokenization of the sample corpus
corpus_words = blob_object.words
print(corpus_words)

print(len(corpus_words))

corpus_sentences= blob_object.sentences
print(corpus_sentences)

print(len(corpus_sentences))

# Pluralization of words using Textblob
from textblob import Word
w = Word('Platform')
w.pluralize()

from textblob import Word
w = Word('Platforms')
w.pluralize()

blob = TextBlob("Great Learning is a great platform to learn data science. \n It helps community through blogs, Youtube, GLA,etc.")
for word,pos in blob.tags:
    if pos == 'NN':
        print (word.pluralize())

blob.tags

# pluralization practice set
from textblob import Word
w=Word('platform')
print(w.pluralize())

p=Word('platforms')
print(w.pluralize())

Text=TextBlob("Great Learning is a great platform to learn data science. \n It helps community through blogs, Youtube, GLA,etc.")
for  word,pos in Text.tags:
  if pos=='NN':
    print(word.pluralize())

blob=TextBlob("Great Learning is a great platform to learn data science. \n It helps community through blogs, Youtube, GLA,etc.")
words=blob.words
for word in words:
  print("ORIGINAL:",word ,"| LEMMA:",word.lemmatize(),"| STEM:", word.stem())

w=Word("learning")
w.lemmatize("v")

# n-gram in Textblob
# An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “not at all”, or “turn off light”.

blob
blob.ngrams(n=1)

blob.ngrams(n=2)

blob.ngrams(n=3)